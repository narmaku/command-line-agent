# LLM Provider Configuration
# Choose one: watsonx, openai, ollama, gemini, anthropic
LLM_PROVIDER=watsonx
LLM_MODEL=ibm/granite-3-8b-instruct

# LLM Parameters
LLM_TEMPERATURE=0.2          # 0.0-1.0 (lower = more deterministic)
LLM_MAX_TOKENS=16000         # Maximum tokens for responses
MEMORY_MAX_TOKENS=12000      # Maximum tokens for conversation memory

# Agent Configuration
AGENT_INSTRUCTIONS_FILE=docs/AGENT_INSTRUCTIONS.md

# Debug Mode
DEBUG=false                  # Set to true for verbose logging

# ===== Provider Credentials =====
# Uncomment and fill in based on your chosen provider

# WatsonX (IBM) Credentials
# WATSONX_API_KEY=your_api_key_here
# WATSONX_PROJECT_ID=your_project_id_here
# WATSONX_URL=https://us-south.ml.cloud.ibm.com

# OpenAI Credentials
# OPENAI_API_KEY=sk-your-key-here

# Google Gemini Credentials
# GOOGLE_API_KEY=your-google-api-key

# Anthropic Claude Credentials
# ANTHROPIC_API_KEY=your-anthropic-key

# Ollama Configuration (for local or remote Ollama)
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_API_BASE=http://localhost:11434
# OLLAMA_CHAT_MODEL=qwen2.5-coder:14b

# ===== Embedding Models (for RAG) =====
# If not set, uses same provider as LLM_PROVIDER
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text

# ===== PostgreSQL Database (for RAG) =====
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_db
# POSTGRES_USER=your_username
# POSTGRES_PASSWORD=your_password

# ===== MCP Server Configuration =====

# Linux MCP Server Path
# LINUX_MCP_SERVER_PATH=/home/user/development/linux-mcp-server

# Allowed log file paths (comma-separated)
# LINUX_MCP_ALLOWED_LOG_PATHS=/var/log/messages,/var/log/secure,/var/log/audit/audit.log

